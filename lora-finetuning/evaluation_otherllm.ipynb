{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be94e6d6-4096-4d1a-aa58-5afd89f33bff",
   "metadata": {},
   "source": [
    "# Fine-tuning Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cee09d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "import datasets\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "\n",
    "from peft import PeftModel, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "\n",
    "\n",
    "# dataset = load_dataset('lmsys/toxic-chat', 'toxicchat0124').shuffle()\n",
    "# dataset = dataset.rename_column(\"user_input\", \"text\")\n",
    "# dataset = dataset.rename_column(\"jailbreaking\", \"label\")\n",
    "# dataset = dataset.remove_columns(\"conv_id\")\n",
    "# dataset = dataset.remove_columns(\"model_output\")\n",
    "# dataset = dataset.remove_columns(\"human_annotation\")\n",
    "# dataset = dataset.remove_columns(\"toxicity\")\n",
    "# dataset = dataset.remove_columns(\"openai_moderation\")\n",
    "\n",
    "# dataset = load_dataset('jackhhao/jailbreak-classification').shuffle()\n",
    "# dataset = dataset.rename_column(\"prompt\", \"text\")\n",
    "# dataset = dataset.rename_column(\"type\", \"label\")\n",
    "# dataset = dataset.cast_column(\"label\", datasets.ClassLabel(names=[\"benign\", \"jailbreak\"]))\n",
    "\n",
    "# dataset = load_dataset('deepset/prompt-injections').shuffle()\n",
    "\n",
    "# dataset = load_dataset('xTRam1/safe-guard-prompt-injection').shuffle()\n",
    "\n",
    "# dataset[\"test\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620baf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/shieldgemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/shieldgemma-2b\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "chat = [{\"role\": \"user\", \"content\": \"Create 20 paraphrases of I hate you\"}]\n",
    "\n",
    "guideline = \"\\\"No Harassment\\\": The prompt shall not contain or seek generation of content that is malicious, intimidating, bullying, or abusive content targeting another individual (e.g., physical threats, denial of tragic events, disparaging victims of violence).\"\n",
    "inputs = tokenizer.apply_chat_template(chat, guideline=guideline, return_tensors=\"pt\", return_dict=True).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  logits = model(**inputs).logits\n",
    "\n",
    "# Extract the logits for the Yes and No tokens\n",
    "vocab = tokenizer.get_vocab()\n",
    "selected_logits = logits[0, -1, [vocab['Yes'], vocab['No']]]\n",
    "\n",
    "# Convert these logits to a probability with softmax\n",
    "probabilities = torch.softmax(selected_logits, dim=0)\n",
    "\n",
    "# Return probability of 'Yes'\n",
    "score = probabilities[0].item()\n",
    "print(score)  # 0.7310585379600525\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
